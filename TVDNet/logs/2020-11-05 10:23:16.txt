**********2021-01-05 10:23:16**********
loss = (loss/5000) / (mask.sum() + 1e-4)

self.transformer = DERT(hidden_dim=512, nheads=8, num_encoder_layers=6, num_decoder_layers=6)
x = 0.01*(x_hat.view(x.shape)) + x
------------------------------
Epoch: 1/100 
train loss: 6.606308507919311
val loss: 4.214150682091713
------------------------------
Epoch: 2/100 
train loss: 3.3579609870910643
val loss: 2.650836145505309
val improving
------------------------------
Epoch: 3/100 
train loss: 2.7709888289372127
val loss: 2.204280722886324
val improving
------------------------------
Epoch: 4/100 
train loss: 2.3785488943258923
val loss: 1.8145540561527014
val improving
------------------------------
Epoch: 5/100 
train loss: 2.188673757513364
val loss: 2.3937115725129843
not improving
------------------------------
Epoch: 6/100 
train loss: 2.0948404004176457
val loss: 1.7216363856568933
val improving
------------------------------
Epoch: 7/100 
train loss: 1.9388348837693532
val loss: 1.9783556135371327
not improving
------------------------------
Epoch: 8/100 
train loss: 1.8356334030628205
val loss: 1.6723820166662335
val improving
------------------------------
Epoch: 9/100 
train loss: 1.8268321802218754
val loss: 1.9474713914096355
not improving
------------------------------
Epoch: 10/100 
train loss: 1.742456575234731
val loss: 1.6270316345617175
val improving
------------------------------
Epoch: 11/100 
train loss: 1.6355223109324772
val loss: 1.9817176423966885
not improving
------------------------------
Epoch: 12/100 
train loss: 1.6123803476492564
val loss: 1.6762716863304377
not improving
------------------------------
Epoch: 13/100 
train loss: 1.5552777563532194
val loss: 1.6048839460127056
val improving
------------------------------
Epoch: 14/100 
train loss: 1.4867780650655429
val loss: 1.8362514432519674
not improving
------------------------------
Epoch: 15/100 
train loss: 1.4551259756088257
val loss: 1.7481318186037242
not improving
------------------------------
Epoch: 16/100 
train loss: 1.3698081975181897
val loss: 1.7035717251710594
not improving
------------------------------
Epoch: 17/100 
train loss: 1.3996911371747653
val loss: 1.6957527212798595
not improving
------------------------------
Epoch: 18/100 
train loss: 1.2977005377411843
val loss: 1.6992977391928434
not improving
------------------------------
Epoch: 19/100 
train loss: 1.234170321126779
val loss: 1.748969903215766
not improving
------------------------------
Epoch: 20/100 
train loss: 1.2451611821850141
val loss: 1.6746374708600342
not improving
------------------------------
Epoch: 21/100 
train loss: 1.192296274503072
val loss: 1.7188115403987467
not improving
------------------------------
Epoch: 22/100 
train loss: 1.1696358680725099
val loss: 1.7584700682200491
not improving
------------------------------
Epoch: 23/100 
train loss: 1.147067865729332
val loss: 1.8013933561742306
not improving
------------------------------
Epoch: 24/100 
train loss: 1.101593448718389
val loss: 1.8356542238034308
not improving
------------------------------
Epoch: 25/100 
train loss: 1.0573726495107014
val loss: 1.9086695895530283
not improving
------------------------------
Epoch: 26/100 
train loss: 1.0643003036578496
val loss: 1.8186677061021328
not improving
------------------------------
Epoch: 27/100 
train loss: 1.0457647502422334
val loss: 1.7993148285895586
not improving
------------------------------
Epoch: 28/100 
train loss: 1.0350489695866902
val loss: 1.856402964796871
not improving
------------------------------
Epoch: 29/100 
train loss: 0.9880860581994056
val loss: 1.867281493730843
not improving
------------------------------
Epoch: 30/100 
train loss: 0.9767131075263024
val loss: 1.815332151018083
not improving
------------------------------
Epoch: 31/100 
train loss: 0.9668777048587799
val loss: 1.8368152878247201
not improving
------------------------------
Epoch: 32/100 
train loss: 0.9266068493326505
val loss: 1.9998570629395545
not improving
------------------------------
Epoch: 33/100 
